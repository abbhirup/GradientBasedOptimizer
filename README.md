# GradientBasedOptimizer
To explore and implement gradient based optimizers

1. Exploring different gradient based optimizers like Gradient descent, SGD, mini batch SGD, Adagrad, Adam, Rmsprop etc. This will help in:

· Understanding the basic concepts of implementation

· Knowing the mathematical equations used for these optimizers.

· Knowing how gradient calculations are done in these optimizers

2. Implementing at least two optimizers starting from scratch. Trying to not use any built in libraries for this implementation. This will help build:

· An Understanding of the equations for each of the optimizer chosen and to help create my own custom functions.

2. Testing each of the optimizers.

· We can test it either by using a known mathematical function or building a simple 3-layer neural network and using a training dataset.

· Comparing the performance of your optimizers.

4. Validating the results with built in functions available in keras.

· Validating the results in task 3 by using built in functions for each optimizer.

· Tabulating the achieved results for custom function and built in function.
